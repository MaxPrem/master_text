---
attachments: [Clipboard_2020-06-10-13-42-37.png, Clipboard_2020-06-10-13-44-47.png, Clipboard_2020-06-12-00-20-13.png]
tags: [masterarbeit]
title: 2.2 Deep Learning
created: '2020-06-10T11:18:59.305Z'
modified: '2020-07-20T22:34:02.874Z'
---

## 2.2 Deep Learning

### Definition
__Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive layers of increasingly meaningful representations. The deep in deep learning isn’t a reference to any kind of deeper understanding achieved by the approach; rather, it stands for this idea of successive layers of representations. How many layers contribute to a model of the data is called the depth of the model. Modern deep learning often involves tens or even hundreds of successive layers of representations— and they’re all learned automatically from exposure to training data. These layered representations are (almost always) learned via models called artificial neural networks (ANNs), structured in literal layers stacked on top of each other.__
[@deeplearning_python]



### Arificial neural networks - nonlinear models
__Use of artificial neural networks (ANNs) for analysis of spectroscopic data have been widely used as nonlinear models for the analysis of spectroscopic data. Convolutional neural networks (CNNs)[2, 3] are ANNs with a special layout and restrictions that make them excellent for modelling spatial data.__ Neighboring data points are analysed in mini networks by “scanning” the data with a limited spatial width (filter size). __This is done multiple times with different mini networks (kernels). The trained weights for the mini network for each kernel must be the same for the entire sweep of the data.__

__ The activations of the kernels are then fed forward in the neural network into new convolutional layers or standard dense layers(basically linear fully conected layers with nonlinear activation functions).__

 The fully connected layers result in a single output neuron with a linear activation function for regression.
 
 [Datauaugmentation - Data Augmentation of Spectral Data for Convolutional Neural Network (CNN) Based Deep Chemometrics]

## Preprocessing & Initialization

### Preprocessing


_According to @deepaug the deep net perfroms best when no baseline correction and transformation is performed as the first layers of the net work learn to perform these pre-processing steps through backprobagation._

```python
aug_pipline = Pipeline([
    ("scaleing_X", GlobalStandardScaler()),
    ("dataaugmentation", Dataaugument()),
    ("scatter_correction", EmscScaler())
    ])    
```

 __ In the classical treatment, raw spectra (output of the instruments) will be preprocessed by these methods before feeding into a calibration framework. The purpose is to remove instrumental noise, and reduce the variance of the spectra from sources such as scatter effect to improve robustness of the calibration model. However, there is no standard rule on choosing proper preprocessing methods.__ The common practice is trial-and-error experimentation.

 Deep Learning can make these steps obsolote.
 
 [Modern practical convolutional neural networks for multivariate regression: Applications to NIR calibration]

 ### DataAugmentation


Instead of correction of the baseline variations we use a special implementation of data augmentation. The idea is to simulate the expected form of irrelevant noise (here baseline offset and slope and overall spectrum intensity), and expect the neural network to either extract features robust to the variations, or figure out the best corrections during training. __This way we can make more efficient use of the labelled dataset__. The idea is similar to the image rotations and croppings done in image recognition training.


The fat blue line is the original spectrum and the others are the modified ones. Lets apply this to all the spectrums and expand the dataset with a factor of 100. I've found that the offset and multiplication works OK around 10% of the standard deviation for the whole dataset. The slopeshift is set to half of this. This could vary depending on the dataset and may be worth experimenting with.




## Model Architecture 
```python

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise (GaussianNois (None, 1168)              0         
_________________________________________________________________
reshape (Reshape)            (None, 1168, 1)           0         
_________________________________________________________________
separable_conv1d (SeparableC (None, 1168, 8)           40        
_________________________________________________________________
activation (Activation)      (None, 1168, 8)           0         
_________________________________________________________________
separable_conv1d_1 (Separabl (None, 1168, 16)          384       
_________________________________________________________________
activation_1 (Activation)    (None, 1168, 16)          0         
_________________________________________________________________
flatten (Flatten)            (None, 18688)             0         
_________________________________________________________________
mc_dropout (MCDropout)       (None, 18688)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               2392192   
_________________________________________________________________
activation_2 (Activation)    (None, 128)               0         
_________________________________________________________________
mc_dropout_1 (MCDropout)     (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 128       
=================================================================
Total params: 2,392,744
Trainable params: 2,392,744
Non-trainable params: 0
_________________________________________________________________
```


### Layers

__Densely connected layer__ also termed fully connected layer (FC layer), linearly maps input vector into another one. Neurons between two adjacent layers are pairwise connected, refer to figure 1.

In our case, the initial inputs are the spectral variables, and the final outputs are the predictions of the target constituent.

__Convolutional layer__  convolves a specific filter with the inputs. The filter is slid over the spectra spatially. We compute the stepwise dot product of the filter with a local window of the spectra, with a stride of 1. We also pad the input spectra to keep the inputs and the outputs at the same size. The convolutional layer is usually followed by fully connected layers, see figure 2 for an example.

![](@attachment/Clipboard_2020-06-10-13-44-47.png)
Figure: A neural network with two hidden layers of 3 neurons each

__Considering that most of the preprocessing methods (e.g., detrend, Savitzky-Golay derivatives) in spectroscopic calibration are equivalent to moving weighted average of the input spectra, we believe that a properly trained convolutional layer can replace a huge range of the classical preprocessing treatments. Since the convolutional layer can be tuned by back-propagation, we do not need to manually choose any specific preprocessing method, but just let the optimization algorithm find the most efficient filter. Since the input vectors are one dimensional spectra, so the filter is also a one dimensional vector. The bandwidth of the filter is relevant to the resolution of the input spectra. It is possible to implement multiple paralleled convolutional channels to increase the flexibility of the model, but accordingly we would need an adequate number of training samples to tune multiple convolutional filters. In this article we only consider single channel (i.e., there is only one convolutional layer) for simplicity.__
### Activation Function

__Outputs of each hidden layer, including the convolutional layer and the fully connected layer, are transformed by the activation function to allow for nonlinearity. In the following discussion, we denote the input to the activation functions as x and the output as y.__

ReLU

SeLU

### Initialization
Proper initialization of the weights in NN ensures fast convergence.__ If the initial weights are too small, then the signal flowing through the network gradually diminishes and eventually becomes meaningless; if the weights are initialized with too large values, the variance of input data into each layer increases rapidly and soon overflows. We hope that the signal variance does not change significantly when passing through the network.__
[Modern practical convolutional neural networks for multivariate regression: Applications to NIR calibration]

### Regularization


if your layer needs to have a different behavior during training and during testing (e.g., if it uses Dropout or BatchNormalization layers), then you must add a train ing argument to the call() method and use this argument to decide what to do. For example, let’s create a layer that adds Gaussian noise during training (for regulariza‐ tion), but does nothing during testing (Keras actually has a layer that does the same thing: keras.layers.GaussianNoise):


#### GaussianNoise
```python
class MyGaussianNoise(keras.layers.Layer): 
  def __init__(self, stddev, **kwargs):
    super().__init__(**kwargs) 
    self.stddev = stddev
  
  def call(self, X, training=None): 
    if training:
      noise = tf.random.normal(tf.shape(X), stddev=self.stddev)
      return X + noise 
    else:
      return X

def compute_output_shape(self, batch_input_shape): 
  return batch_input_shape
``````


__Early Stopping__ __ is a very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. Figure 4-20 shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set naturally goes down, and so does its prediction error on the validation set.__

```python
class CustomStopper(keras.callbacks.EarlyStopping):
    '''preventing too early stopping by setting start.epoch'''
    def __init__(self, monitor='val_loss',
             min_delta=0, patience=0, verbose=0, mode='auto', start_epoch = 100): # add argument for starting epoch
        super(CustomStopper, self).__init__()
        self.start_epoch = start_epoch

    def on_epoch_end(self, epoch, logs=None):
        if epoch > self.start_epoch:
            super().on_epoch_end(epoch, logs)
```

___Dropout__ is a simple way to prevent neural networks from overfitting [20]. The idea can be illustrated by figure 3. __During training, we randomly activate a proportion of neurons in FC layers, and only update the parameters of the selected neurons.__ During testing no dropout is applied. Dropout is simple but efficient. __It is widely used in modern practical neural networks.__

### LossFunction

__Huber Loss__
```python
def huber(y_true, y_pred, delta=1.0):
	y_true = y_true.reshape(-1,1)
	y_pred = y_pred.reshape(-1,1)
	return np.mean(delta**2*( (1+((y_true-y_pred)/delta)**2)**0.5 -1))
```

### Optimization

__Back-propagation__ is the basic optimization method used in training neural networks [22]. __It calculates the error contribution of each neuron by evaluating some training samples (normally a batch of data drew from the whole training set). It is normally used by the gradient descent optimization algorithms (including the Adam optimizer [23], which is used in this study) to tune the parameters in the model. The gradient of the loss function is calculated and then distributed backwards through the whole network.__

__Adam optimization__ __is a recently proposed optimizer for neural networks [23], and is often considered the best optimizer currently available. It has many beautiful features, such as rescaling invariance, suitability for non-stationary loss function, and automatic learning rate annealing. When using the Adam optimizer, we need to predetermine 4 hyperparameters: β1 (decay pa- rameter for the gradient, normally set to 0.9); β2 (decay parameter for the squared gradient, normally set to 0.999); α (learning rate) and ε (a very small number to prevent division by 0, for example 10−6). Adam is often recommended as the default optimizer.__

There is also a nice research benchmarking other popular stochastic optimizers [24] as optional choices.


### Learning Rate

If no training improvement occures after a defined period, the learning rate is halfed.
![](@attachment/Clipboard_2020-06-12-00-20-13.png)

```python
rdlr = ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6, monitor='val_loss', verbose=1)
```
