---
attachments: [Clipboard_2020-07-31-18-10-30.png]
tags: [masterarbeit]
title: 2.2 Deep Learning
created: '2020-07-31T15:18:26.920Z'
modified: '2020-08-04T19:16:09.147Z'
---

# ## 2.2 Deep Learning

### Definition
Deep Learning is a fast growing subfield of machine learning, achieving unprecedented results in fields as natural language processing, image detection, time series forecast and also regression on chemometric data. Deep neural networks are artificial neural networks with many consecutive, connected layers, giving the model depth to learn higher levels of abstraction from the extracted input features. The amount of layers needed depends on the complexitiy of the data, the amount of data, and the task/requirements of the model. Classical tasks include classification, regression, anomaly detecion, object detection, self-driving cars, but also generation of new data like fake media that can't be distinguished easily as such.
[@deeplearning_python] [@cnnbrain]



### Arificial neural networks as nonlinear regression models

Nonlinear behaviour of NIR spectroscopic data from diffuse reflectance is a well known problem, that can be adjust to some point by the use of preprocessing algorithms, forcing the data to obey Lambert-Beers's law. 

Convolutional neural networks, deep neural nets with preceding convolutional layers, excel at mapping and extracting features from spatial data, such as pictures and spectra. Neighboring data points are analysed in mini networks (kernels) by “scanning” the data with a limited spatial width (filter size).
The processed features are feed to densely connected layers (or another convolutional layer), learning the associated relationships from the given higher level abstractions. Non-linear behaviour is achieved by non-linear activation functions, triggerd at a certain treshold to pass information to adjacent neurons. One well-known nonlinear activation function is the sigmoid function, used in logistic regression. 
The successional final output neuron, condensed from the highly conected layers, uses a linear activation function to predict a continous response variable. [@Krizhevsky2017] [@cnnbrain]
If the task is classification, the final output neurons resemble the to be predicted classes and categorical crossentropy is used as a loss function to adjust the wheights instead of a residual loss functions like mean squared error for regression.
[@handson8]


![](@attachment/Clipboard_2020-06-10-13-44-47.png) [@Cui2018]

### Black Box model

A CNN can be viewed as a black box model. It receives data as inputs and generates outputs. The internal structure remains hidden and the parameters of the CNN are adjusted so that a behavior between input and output is learned and mapped, without the actual causal relationships, i.e. Knowing cause-effect relationships. The CNN is therefore a model for the approximation of any functional relationships.
The higher level of complexity results in a model with higher predictive power, but it is also less interpretable. 

![](@attachment/Clipboard_2020-07-31-12-35-42.png)
![](@attachment/Clipboard_2020-07-31-12-35-58.png)
![](@attachment/Clipboard_2020-07-31-12-36-05.png)

According to Occam there is a conflict between simplicity and accuracy. Logistic Regression may lead to an easy interpretable model, but it can not handle multivariate and highly correlated data. PLS sacrifices some of its interpretability by creating linear combinations of the input features, so called latent variables. If there is too much noise is present in the data the capability of this procedure to explainn the underlying variance is lessend.
When interpreting noise as a set of not meassured variables, the strength of DNN becomes clear. With millions of weights to fine tune, the model is able to emulate the process creating the data effectively.

[@Breiman2001]

## Model Architecture 
```python
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise (GaussianNois (None, 1951)              0         
_________________________________________________________________
reshape (Reshape)            (None, 1951, 1)           0         
_________________________________________________________________
separable_conv1d (SeparableC (None, 1951, 8)           40        
_________________________________________________________________
activation (selu)            (None, 1951, 8)           0         
_________________________________________________________________
mc_dropout (MCDropout)       (None, 1951, 8)           0         
_________________________________________________________________
separable_conv1d_1 (Separabl (None, 1951, 16)          384       
_________________________________________________________________
activation_1 (selu)          (None, 1951, 16)          0         
_________________________________________________________________
flatten (Flatten)            (None, 31216)             0         
_________________________________________________________________
mc_dropout_1 (MCDropout)     (None, 31216)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               3995776   
_________________________________________________________________
activation_2 (selu)          (None, 128)               0         
_________________________________________________________________
mc_dropout_2 (MCDropout)     (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 128       
_________________________________________________________________
activation_3 (linear)        (None, 1)                 0         
=================================================================
Total params: 3,996,328
Trainable params: 3,996,328
Non-trainable params: 0
________________________

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise (GaussianNois (None, 1168)              0         
_________________________________________________________________
reshape (Reshape)            (None, 1168, 1)           0         
_________________________________________________________________
separable_conv1d (SeparableC (None, 1168, 8)           40        
_________________________________________________________________
separable_conv1d_1 (Separabl (None, 1168, 16)          384       
_________________________________________________________________
flatten (Flatten)            (None, 18688)             0         
_________________________________________________________________
mc_dropout (MCDropout)       (None, 18688)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               2392192   
_________________________________________________________________
mc_dropout_1 (MCDropout)     (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 128       
=================================================================
Total params: 2,392,744
Trainable params: 2,392,744
Non-trainable params: 0
_________________________________________________________________
```

### Summary Architecture

The model is also called a sequential model as we can add the layers sequentially, layer after layer.

#### Input Size

The input size is determined by the number of wavelenghts of the spectra used for the analysis. As we are working with spectra, only containing absorbance and wavenumbers, the data can be viewed as one-dimensional pictures. 

#### Gaussian Noise

Gaussian Noise performs a form of data augmentation by adding noise during training, helping the model to generalise better on the input data to mitigate overfitting. The regularization layer is only active during training. [@gnoise2020]

#### Reshape

The spectral tensors are extended by one dimension to store the following additional channels from the convolutional layers.

#### Convolutional layer

Convolutional layers are filters preceeding a neural network and are highly effictive at extracting features from visual data. 
By using a moving filter the structures in the data are feed to folding and pooling layers, extracting relevant information for the following fully connected layers. The information can also be forwarded to another convolutional layer, learning higher levels of abstractation from the input features, enabeling the network to learn many subtle variations in the visual data.
The type of filter used is not specified, but learned form the network by error minimalization through backpropagation, making neural networks also a highly effective technique to  preprocess spectra.
 
![](@attachment/Clipboard_2020-07-30-23-16-40.png)

[@Guo2019]

The use of sperable convolutional filters is recomended over convolutional filters, as they can also learn from the depth in the spatial data. This layer performs a depthwise convolution that acts separately on channels,followed by a pointwise convolution that mixes channels. The first convolutional layer operates with 8 kernels,specifying the spatial dimensions of the filters.
The second convolutional layer runs with 16 kernels. The filtersize for both layers is set so 32.
[@tfconv2020]

#### Flatten

Before feeding our convolved spectra into the dense layers we need to flatten them, as they only take vector shaped data as input. Flattening our spectra with 16 spatial dimension results into a vector 16 times our original input size. The output signals of the filter layers are now independent of the position, so there are no longer any positional characteristics, but location-independent object informations.

#### Dropout

Dropout acts as an effective regularization technique deactivating neurons randomly during training, resulting in more robustly calculated weights as the neurons can no longer depend on their neighbours to make meaningful predictions.

![](@attachment/Clipboard_2020-07-31-14-19-25.png)
[@Sapienza2008]

Monte Carlo dropout is active during training and testing, opposed to normal dropout only beeing active during training. The predictions made are no longer deterministic, as they now depend on the randomly activated neurons.
The resulting predictions follow a distribution that can be interpreted as a Bayesian approximation of a Gaussian process. Makeing Monte Carlo dropout not only a regularization method, but also a tool to estimate uncertainty for already trained models. Beeing able to calculate confidence intervalls without having to retrain the model on a new subset of data gives a huge advantage. Makeing 500 predictions from one model with MC Dropout is by far less computational expensive compared to training 10 neural nets for cross validation to calculate uncertainty.
[@Gal2016]

#### Dense Layer
The fully connected layers or dense layers are typical neural network structure in which all neurons are connected to all input and all output neurons (see picture from dropout). In order for the linear connections to learn non-linear relationships, nonlinear activation functions are needed, using a treshold to decide if a neuron activates and transducts signal to the adjacent neurons.

#### Final Output Neuron

The final output neuron condensed from the highly conected layers, uses a linear activation function (instead of a non linear) to predict a continous response variable. 

 
## Initializing the Model

```
model.compile(loss=HuberLoss(), optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999))
```

After creating our model we need to initialize it before we can start the trainig. Initalizing the model means assigning random values to the weights in the layers that can be adjusted by the defined optimzer (Nadam) during backpropagation to minimize the defined cost function (HuberLoss). (see Optimization)

The initalization method depends on the activation function used, as certain values (close to 0 and negative values) can lead to dead neurons with some activation functions (ReLu), which can not be updated by the optimizer during training. For ReLu we use the HE-normal initializer.

Like the Glorot initializer He-normal defines weights by drawing them from a distribution with 0 mean and an estimated variance.
Additionally, he-normal multiplies the weights by a factor considering the shape and position of the layers initialized, resulting in a faster converging model.

### Loss Function

The loss function also called cost function, depends on the task for the network trained. For regression we can use the mean squared error or another residual loss function. If outliers are present in the data, the Huber loss function performs much better by scaling the influence of outliers down at a defined treshold, reducing their quadratic influence. [@handson8]
The calculated errors are used by the optimizer during backpropagation to update the weights of the neurons, by a small amount defined by the learning rate, in the direction reducing overall loss.

### Activation Function

Activation functions calculate the output of a neuron and decide if a neuron becomes active or not, based on the relevance of a neuron to the final output. A well selected activation function leads to faster converging networks and should be fast to calculate, as it is used in almost all neurons.

#### ReLU
Rectified lienar unit (ReLu) is a popular non-linear activation function and can be used for back propagation. A great advantage of the ReLu function is that not all neurons are active at the same time, by reducing all negative values to zero. These neurons are deactivated, which brings speed advantages.
However, there is also a problem with ReLu: With negative values, the gradient is zero. This can create "dead" neurons that are never activated, as a slope is needed for a gradient to decent in order to update the weights. [@handson8] If the slope of the activation function is to small the vanishing Gradient problem occures, as a change in one or the other direction leads to (almost) the same result and no progress in training occures.
![](@attachment/Clipboard_2020-07-31-17-05-19.png) [@relu20]

#### Leaky ReLu
Leaky ReLu is an improved version of ReLu that solves the problem with the "dead" neurons. Instead of the constant zero value for negative values, a straight line is used. The slope of this straight line is a parameter that can be trained (and optimized) by the CNN. The activation function is also called parameterized ReLu. [@handson8]

![](@attachment/Clipboard_2020-07-31-17-05-06.png) [@relu20]

ReLu and Leaky ReLu are a good choice for CNN as they lead to a faster convergeing loss (by setting some of the neurons to 0) than other activation functions  in combination with convolutional layers.

### Optimization

The optimizer has the task of updating the weights and bias ​​of the network in a way that minimize the error of the loss function to find a global minimum, representing an optimal solution.

In order to do this, TensorFlow calculates and tracks the so-called gradients of the cost function, which indicate the direction in which the weights and bias values ​​have to be adjusted in order to minimize the cost function of the model. The development of fast and stable optimizers is a large branch of research in the area of ​​neural networks and deep learning.

One famous but outdated optimizer is called Gradient Decent. Here the so-called keras.optimizers.Nadam is used, which stands for Adaptive Moment Estimation and is considerd the best optimizer currently available.  


```
model.compile(loss=HuberLoss(), optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999))
```

Keras and Tensorflow have already optimised hyperparameters that are sufficient for many tasks. With enough computational resources, model parameters can be optimized using sklearns GridSearchCV class, training a handfull of models with a list of parameter values.

 4 hyperparameters: 
 * β1 decay parameter for the gradient, normally set to 0.9
 * β2 decay parameter for the squared gradient, normally set to 0.999
 * α learning rate set to 0.001
 * ε a very small number to prevent division by 0, for example 10−6

### Learning Rate

Optimizer work by tracking the gradient of their assigned loss function and trying to find a global minium. The magnitude of the adjustment is defined by the learning rate. A learning rate to high will jump around the minium without finding it, a learning rate to low will likely get stuck in a local minium instead of the desired global minimum. [@handson8]

![](@attachment/Clipboard_2020-07-31-18-10-30.png)

[@jordan2018]

#### Optimizing Learning Rate during training

If no training improvement occures after a defined period, the learning rate is halfed. This method is called reduce on plateau. [@handson8]

![](@attachment/Clipboard_2020-06-12-00-20-13.png)
_x-Achsis: number of epoches; y-Achsis: learning rate_

In this screenshot from tensorboard the updates of the learning rate starting form 0.001 are made visible from different runs. Each time no training improvement occures for a defined period, the learning rate is halfed.

```python
rdlr = ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6, monitor='val_loss', verbose=1)
```

## Training the model
```python
history = model.fit(X_aug, y_aug, epochs = 70, batch_size=16, validation_data=(X_test, y_test))
```


#### Forward Pass

The input data are send through the neural network at one side, and on the other side the errors are estimated by the loss function comparing calculated response variables with meassured ones.

Each input signal is distributed to each individual neuron in the first layer. Each neuron then weights the incoming signal with an input-specific weight (which was assigned randomly at the beginning), and adds a neuron-specific bias term (erors) and sums up all the weighted input data to the output of this one neuron.

The output is processed by a non-linear activation function, which determines if the neuron becomes active or not. The output of each active neuron is then passed on as input to all neurons in the following layer. This process continues until the final output neuron is reached, returning the final output value.


#### Calculate Errors

After the first forward pass neural network is still untrained. The final outputvale is compared to the training data (labels), and a loss function calculates the size of the error.


#### Backward Pass


 The measured error is fed backwards into the artificial neural network (the so-called backward pass or backward propagation), and each weight and each bias term is adjusted according to the learning rate in the direction that corresponds to make the error smaller.

#### Batches and Epoches

A subset of test data is also called a batch and is determined by the batch-size, an important hyperparameter. Sending all training and test data through the network is called an epoch. Not sending all the samples through the  network at once has the advantage of saving the computers working memory. A batch size to small has the disadvantage that, the samples mean and standardeviation fluctuate more from batch to batch, make training less effective.

## Regularization

Methods for regularizing ANN to prevent overfitting have already been mentioned, but are summarized here again.

### Gaussian Noise

Gaussian Noise performs a form of data augmentation by adding noise during training, helping the model to generalise better on the input data to mitigate overfitting. The regularization layer is only active during training. [@gnoise2020]

### Dataaugmentation

Dataaugmentation makes it possible to multiply the training data by a factor of for example 100  and increases training succes greatly, especially if their is not a lot of traingdata. 

We call the augmented trainingsdata X_aug and y_aug. The reference method y_aug is simply multiplied by 100. The trainingsspectra on the other hand, are transformed to simulate noise from diffuse reflectance meassurements and smoothed afterwards using Extended Multiplicative Scattercorrection. The augmentated spectra improve the convolutionallayers ability to learn effective preprocessing transformations and improve overall feature extraktion.


### Dropout

Dropout acts as an effective regularization technique deactivating neurons randomly during training, resulting in more robustly calculated weights as the neurons can no longer depend on their neighbours to make meaningful predictions. The dropout parameter determines how many neuron stay active, a good standard value is 50%.

Methods like Dropout prevent overfitting by randomly deactivating 50% of the neurons during training.This has the effekt that the neurons become more robust to noise and predict more independtly. 
### Early stopping

Earlty stopping monitors the test loss and stops trainig if no improvement occures after a defined peroid of epoches.
This saves computational resources and prevents overfitting.
As the wheights are saved from time to time during training, the weights that resulted in the highest accuracy can be loaded after stopping the training.


