---
attachments: [Clipboard_2020-06-10-13-42-37.png, Clipboard_2020-06-10-13-44-47.png, Clipboard_2020-06-12-00-20-13.png, Clipboard_2020-07-30-23-16-40.png, Clipboard_2020-07-31-12-35-42.png, Clipboard_2020-07-31-12-35-58.png, Clipboard_2020-07-31-12-36-05.png, Clipboard_2020-07-31-14-19-25.png, Clipboard_2020-07-31-17-03-24.png, Clipboard_2020-07-31-17-05-06.png, Clipboard_2020-07-31-17-05-19.png]
deleted: true
pinned: true
tags: [masterarbeit]
title: 2.2 Deep Learning
created: '2020-06-10T11:18:59.305Z'
modified: '2020-07-31T15:18:08.902Z'
---

## 2.2 Deep Learning

### Definition
Deep Learning is a fast growing subfield of machine learning, achieving unprecedented results in fields as natural language processing, image detection, time series forecast and also regression on chemometric data. Deep neural networks are artificial neural networks with many consecutive, connected layers, giving the model depth to learn higher levels of abstraction from the extracted input features. The amount of layers needed depends on the complexitiy of the data, the amount of data, and the task/requirements of the model. Classical tasks include classification, regression, anomaly detecion, object detection, self-driving cars, but also generation of new data like fake media that can't be distinguished as such.
[@deeplearning_python] [@cnnbrain]



### Arificial neural networks as nonlinear regression models

Nonlinear behaviour of NIR spectroscopic data from diffuse reflectance is a well known problem, that can be adjust to some point by the use of preprocessing algorithms, forcing the data to obey Lambert-Beers's law. 

Convolutional neural networks, deep neural nets with preceding convolutional layers, excel at mapping and extracting features from spatial data, such as pictures and spectra. Neighboring data points are analysed in mini networks (kernels) by “scanning” the data with a limited spatial width (filter size).
The processed features are feed to densely connected layers (or another convolutional layer), learning the associated relationships from the given higher level abstractions. Non-linear behaviour is achieved by non-linear activation functions, triggerd at a certain treshold to pass information to adjacent neurons. One well-known nonlinear activation function is the sigmoid function, used in logistic regression. 
The successional final output neuron, condensed from the highly conected layers, uses a linear activation function to predict a continous response variable. [@Krizhevsky2017] [@cnnbrain]
If the task is classification, the final output neurons resemble the to be predicted classes and categorical crossentropy is used as a loss function to adjust the wheights instead of a residual loss functions like mean squared error for regression.
[@handson8]


![](@attachment/Clipboard_2020-06-10-13-44-47.png)

### Black Box model

A CNN can be viewed as a black box model. It receives data as inputs and generates outputs. The internal structure remains hidden and the parameters of the CNN are adjusted so that a behavior between input and output is learned and mapped, without the actual causal relationships, i.e. Knowing cause-effect relationships. The CNN is therefore a model for the approximation of any functional relationships.
The higher level of complexity of DNN compared to logistic regression results in a model with higher predictive power, but is less interpretable. According to Occam there is a conflict between simplicity and accuracy.

![](@attachment/Clipboard_2020-07-31-12-35-42.png)
![](@attachment/Clipboard_2020-07-31-12-35-58.png)
![](@attachment/Clipboard_2020-07-31-12-36-05.png)

[@Breiman2001]

## Model Architecture 
```python
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise (GaussianNois (None, 1951)              0         
_________________________________________________________________
reshape (Reshape)            (None, 1951, 1)           0         
_________________________________________________________________
separable_conv1d (SeparableC (None, 1951, 8)           40        
_________________________________________________________________
activation (selu)            (None, 1951, 8)           0         
_________________________________________________________________
mc_dropout (MCDropout)       (None, 1951, 8)           0         
_________________________________________________________________
separable_conv1d_1 (Separabl (None, 1951, 16)          384       
_________________________________________________________________
activation_1 (selu)          (None, 1951, 16)          0         
_________________________________________________________________
flatten (Flatten)            (None, 31216)             0         
_________________________________________________________________
mc_dropout_1 (MCDropout)     (None, 31216)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               3995776   
_________________________________________________________________
activation_2 (selu)          (None, 128)               0         
_________________________________________________________________
mc_dropout_2 (MCDropout)     (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 128       
_________________________________________________________________
activation_3 (linear)        (None, 1)                 0         
=================================================================
Total params: 3,996,328
Trainable params: 3,996,328
Non-trainable params: 0
________________________

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise (GaussianNois (None, 1168)              0         
_________________________________________________________________
reshape (Reshape)            (None, 1168, 1)           0         
_________________________________________________________________
separable_conv1d (SeparableC (None, 1168, 8)           40        
_________________________________________________________________
separable_conv1d_1 (Separabl (None, 1168, 16)          384       
_________________________________________________________________
flatten (Flatten)            (None, 18688)             0         
_________________________________________________________________
mc_dropout (MCDropout)       (None, 18688)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               2392192   
_________________________________________________________________
mc_dropout_1 (MCDropout)     (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 128       
=================================================================
Total params: 2,392,744
Trainable params: 2,392,744
Non-trainable params: 0
_________________________________________________________________
```

## architecture - summary

#### input size

The input size is determined by the number of wavelenghts of the spectra used for analysis. As we are working with spectra, only containing absorbance and wavenumbers, the data can be treated as one-dimensional pictures. 

#### Gaussian Noise

Gaussian Noise performs a form of data augmentation by adding noise during training, helping the model to generalise better on the input data to mitigate overfitting. The regularization layer is only active during training.

#### Reshape

The spectral tensors are extended by one dimension to store the following channels from the convolutional layers.

#### Convolutional layer

Convolutional layers are filters preceeding a neural network and are highly effictive at extracting features from visual data. 
By using a moving filter the structures in the data are feed to folding and pooling layers, condensing relevant information for the following fully connected layers. The information can also be forwarded to another convolutional layer, learning higher level abstractations of the input features, making it possible to learn many subtle variations in the visual data.
The type of filter used is not specified, but learned form the network by error minimalization through backpropagation. Making neural networks also a highly effective technique to  preprocess spectra.
 
![](@attachment/Clipboard_2020-07-30-23-16-40.png)
_Convolutional filter and seperable convolutional filter compared_
[@Guo2019]

The use of sperable convolutional filters is recomended over convolutional filters, as they can also learn from the depth in the spatial data. This layer performs a depthwise convolution that acts separately on channels,followed by a pointwise convolution that mixes channels. The first convolutional layer operates with 8 kernels,specifying the spatial dimensions of the filters.
The second convolutional layer runs with 16 kernels. The filtersize for both layers is set so 32.
[@tfconv2020]

#### Flatten

Before feeding our convolved spectra into the dense layers we need to flatten them, as they only take vector shaped data as input. Flattening our spectra with 16 spatial dimension results into a vector 16 times our original input size. The output signals of the filter layers are now independent of the position, so there are no longer any positional characteristics, but location-independent object information.

#### Dropout

Dropout acts as an effective regularization technique deactivating neurons randomly during training, resulting in more robustly calculated weights as the neurons can no longer depend on their neighbours to make meaningful predictions.

![](@attachment/Clipboard_2020-07-31-14-19-25.png)
[@Sapienza2008]

Monte Carlo dropout is active during training and testing, opposed to normal dropout only beeing active during training. The predictions made are no longer deterministic, as they now depend on the randomly activated neurons.
The resulting predictions follow a distribution that can be interpreted as a Bayesian approximation of a Gaussian process. Makeing Monte Carlo dropout not only a regularization method, but also a tool to estimate uncertainty for already trained models. Beeing able to calculate confidence intervalls without having to retrain the model on a new subset of data gives a huge advantage. Computing 500 predictions from one model with MC Dropout is by far less computational expensive compared to training 10 neural nets for cross validation to calculate uncertainty.
[@Gal2016]

#### Dense 
The fully connected layers or dense layers are typical neural network structure in which all neurons are connected to all inputs and all outputs. In order for the linear connections to learn non-linear relationships, activation functions are needed, creating a treshold deciding when to activate a neuron and transduct signal to the adjacent neurons.

#### Final Output Neuron

The final output neuron condensed from the highly conected layers, uses a linear activation function (instead of a non linear) to predict a continous response variable. 

 
## Initializing the Model

After creating our model shown in the above summary we need to initialize it before we can start the trainig. Initalizing the model means assigning values to the weights in the layers that can be adjusted by the optimzer during backpropagation. (see Optimaization)

_, by tracking the gradient of their assigned loss function and trying to find a minium.The magnitude of the adjustment is defined by the learning rate. A learning rate to high will jump around the minium without finding it, a learning rate to low will likely get stuck in a local minium instead of the desired global minimum._

The initalization method depends on the activation function used, as some values can lead to dead neurons which can not be updated during training. For ReLu we use the HE Normal initializer.

Like the Glorot Initializer he-normal 
he-normal defines weights in a way that help to find a minimum on the cost function faster and more efficently, by considering the shape and position of the layers initialized._

### Activation Function


_ReLU (rectified linear unit) a poplar activation function, suffers form neurons set to 0, as the gradient for this neuron is also 0 and no minima can be found by the optimzer. The associated initalization method he-normal defines weights in a way that help to find a minimum on the cost function faster and more efficently, by considering the shape and position of the layers initialized._

#####
ReLu is non-linear and can be used for back propagation. A great advantage of the ReLu activation function is that not all neurons are active at the same time. Since all negative values are reduced to zero by ReLu, these neurons are deactivated, which brings speed advantages.
For these reasons, ReLu is the most frequently used activation function. However, there is also a problem with ReLu: With negative values, the gradient is zero. This can create "dead" neurons that are never activated.
![](@attachment/Clipboard_2020-07-31-17-05-19.png)

Leaky ReLu
######
Leaky ReLu is an improved version of ReLu that solves the problem with the "dead" neurons. Instead of the constant zero value for negative values, a straight line is used. If the slope a of this straight line is a parameter that can be trained by the KNN, the activation function is also parameterized ReLu.

![](@attachment/Clipboard_2020-07-31-17-05-06.png)

### Optimization
The optimizer has the task of adapting the weights and bias values ​​of the network during the training on the basis of the calculated model deviations of the cost function. To do this, TensorFlow calculates so-called gradients of the cost function, which indicate the direction in which the weights and bias values ​​have to be adjusted in order to minimize the cost function of the model. The development of fast and stable optimizers is a large branch of research in the area of ​​neural networks and deep learning.

Here the so-called keras.optimizers.Nadam is used, which stands for Adaptive Moment Estimation and is considerd the best optimizer currently available.

```
model.compile(loss=HuberLoss(), optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999))
```

 4 hyperparameters: 
 * β1 decay parameter for the gradient, normally set to 0.9
 * β2 decay parameter for the squared gradient, normally set to 0.999
 * α learning rate set to 0.001
 * ε a very small number to prevent division by 0, for example 10−6

 At this point, we do not go into the mathematical details of the optimizers, as this would clearly go beyond the scope of this introduction. It is important to understand that there are different optimizers that are based on different strategies for calculating the necessary adjustments of the weighting and bias values.


## Training

history = model.fit(X_aug, y_aug, epochs = 70, batch_size=16, validation_data=(X_test, y_test) , callbacks=[rdlr, custom_stopper, tensorboard_cb, checkpoint_cb])

#### epoch training...
forward pass

The input data are fed in on one side of the neural network. Each input signal is distributed to each individual neuron in the first layer. Each neuron then weights the incoming signal with an input-specific weight (which was assigned randomly at the beginning!), Adds a so-called neuron-specific bias term and sums up all the weighted input data to the output of this one neuron.

Often the output is guided by a non-linear activation function, e.g. to to force a certain value range of the output (e.g. Sigmoid, tanh, etc.). The output of each neuron is then passed on as input to all neurons in the following layer. This process continues until the output layer is reached, which provides the result of all calculations.

#### calculate errors

The still untrained neural network calculates a result for a set of input data (also called features). This result is then compared with the known results of the example data set (also called targets or labels) and the size of the deviation or error is calculated. In order to be able to map both positive and negative deviations at the same time, for example the mean value of the squared mean error (SME) or another error function is used.


#### backward pass


Then the actual "learning" begins: The measured error is fed backwards back into the artificial neural network (the so-called backward pass or backward propagation), and each weight and each bias term is adjusted a little bit in the direction that corresponds to the error makes smaller. The size of this adjustment is firstly calculated via the proportion that a specific neuron weight had in the result (i.e. via its current weight), and secondly via the so-called learning rate, which is one of the most important setting parameters (hyper parameters) of neural networks.

Common learning rates are e.g. 0.001 or 0.01, i.e. only one hundredth to one thousandth of the calculated error is corrected per run. If the adjustment per run is too large, the minimum of the error curve may be missed and the deviations may become larger instead of smaller (overshoot). Sometimes the learning rate is therefore increasingly reduced during the training (learning rate decay) in order to better determine the minimum of the error function.

Another possible problem is error functions with local minima, in which the neural network “gets stuck” and therefore cannot find the actual minimum. The direction of the correction is calculated by deriving the respective functions, the negative value of which specifies the direction in which the error function is minimized. The goal of training or learning is to minimize the selected error function.

__Back-propagation__ is the basic optimization method used in training neural networks [22]. __It calculates the error contribution of each neuron by evaluating some training samples (normally a batch of data drew from the whole training set). It is normally used by the gradient descent optimization algorithms (including the Adam optimizer [23]) to tune the parameters in the model. The gradient of the loss function is calculated and then distributed backwards through the whole network.__



### Learning Rate

###########

3.2.3 Learning rate
The learning rate influences the step size of the gradient descent algorithm (see Ng 2018). In the case of gradient descent, the step size is also dependent on the gradient, which is why it is not necessary to change the learning rate during training: if the algorithm approaches a local minimum, the gradient decreases and the step size becomes smaller. Nevertheless, choosing an appropriate learning rate is important. With a very low learning rate, the step size of the algorithm is very small and the algorithm works very slowly. If the learning rate is too high, it can happen that gradient descent skips the local minimum, in the worst case even so far that the algorithm moves further away from the local minimum with each step.


If no training improvement occures after a defined period, the learning rate is halfed.
![](@attachment/Clipboard_2020-06-12-00-20-13.png)

```python
rdlr = ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6, monitor='val_loss', verbose=1)
```

### Aktivation function

As an activation function, the network input is applied to the sum of the weighted input values ​​in order to obtain the output, the activity level of the neuron.

 If the input is large enough, the unit is activated and fires. In the simplest case it is a linear function, otherwise it is a piecewise linear, a step function or nonlinear.  The latter is used particularly often.
 
  A linear function offers little flexibility and is only suitable for simple CNNs or as final activation function for a single output neuron in regression applications.
  
A piecewise linear function is limited in its scope. A step function, also called a threshold function, returns zero for negative input values ​​and one for positive ones.




The main advantage of the ReLu function is that when calculating CNNs they have no problems with the “shrinking” of the gradient, as can occur with the sigmoid function. Furthermore, it has been found that CNNs can be trained more efficiently using ReLu [1,5,6].

#####
ReLu is non-linear and can be used for back propagation. A great advantage of the ReLu activation function is that not all neurons are active at the same time. Since all negative values are reduced to zero by ReLu, these neurons are deactivated, which brings speed advantages.
For these reasons, ReLu is the most frequently used activation function. However, there is also a problem with ReLu: With negative values, the gradient is zero. This can create "dead" neurons that are never activated.

Leaky ReLu
######
Leaky ReLu is an improved version of ReLu that solves the problem with the "dead" neurons. Instead of the constant zero value for negative values, a straight line is used. If the slope a of this straight line is a parameter that can be trained by the KNN, the activation function is also parameterized
Called ReLu.



### Initialization
Proper initialization of the weights in NN ensures fast convergence.__ If the initial weights are too small, then the signal flowing through the network gradually diminishes and eventually becomes meaningless; if the weights are initialized with too large values, the variance of input data into each layer increases rapidly and soon overflows. We hope that the signal variance does not change significantly when passing through the network.__
[Modern practical convolutional neural networks for multivariate regression: Applications to NIR calibration]

### Regularization


if your layer needs to have a different behavior during training and during testing (e.g., if it uses Dropout or BatchNormalization layers), then you must add a train ing argument to the call() method and use this argument to decide what to do. For example, let’s create a layer that adds Gaussian noise during training (for regulariza‐ tion), but does nothing during testing (Keras actually has a layer that does the same thing: keras.layers.GaussianNoise):

## Regularization 

