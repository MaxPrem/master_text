---
tags: [masterarbeit]
title: 2.3 Preprcoessing
created: '2020-06-24T12:17:23.969Z'
modified: '2020-08-04T21:03:51.468Z'
---

## 2.3 Preprcoessing

## 2.3 Signal Processing

Before analyzing the obtained NIR spectra, signal processing algorithms are applied to remove meassureing artefacts, reduce unwanted variance and try to make the data follow Lambert-Beers law to improve the overall predictive accuracy. [@Rinnan2009]  The light scattering during meassuring in diffuse reflectance causes non-linearity. Also, noise originating from the environment and the sample's phyisical properties result in baseline- and slopesshifts, random fluctuations, varying peak intensities and uninformative atmospheric peaks associated with water vapor and CO$_{2}$.
 
 The objective is to improve the general signal to noise ratio by using scatter correcting algorithms, smoothing filters and mathematical transformations as mean-centering and derivatisation. 
 The order in which these functions are applied can greatly influence the outcome of a regression analysis. Preprocessing the spectra to intensely can blow up noise and leads to overfitting. [@Stevens2014]

```python

# sklearn Pipline containing preprocessing steps
  pipe = Pipeline([
      ("scaleing_X", GlobalStandardScaler()),
      ("scatter_correction", EmscScaler()),
      ("smmothing", SavgolFilter(polyorder=2,deriv=0)),
      ("variable_selection", Enet_Select())
  ])

# train and test spectra are pre-processed with the fit_transform and transform method
  pipe.fit_transform(X_train, y_train)
  pipe.transform(X_test)
```
* In the first step NIR spectra are scaled using EMSC or GlobalStandardScaler to unit standard deviation and mean centered using _global mean_ and $std$ of $X$, 

* EMSC removes scattering artifacts like baseline shifts and varied slopes .

* Next spectra are smoothed using the Savgol Filter. Derivatisation can be performed in the same step to remove the baseline, although a varying baseline can contain viable information.

* In the final step Elastic Net performs variable selection by regularized regression trying to create a sparse model by selecting a set of informative and not highly intercorrelated varaibles, correlating with the reference method.

* Steps like mean-centering and derivatisation can amplify noise and should be used with care as baseline information is lost.

* Finding an effective recipe for preprocessing depends strongly on the data obtained, the regions recorded and presence/absence of uninformative peaks in the spectra. Variable selection can help to find reagions of great intrest, makeing it possible to discard 90% of the spectra if the full NIR region was recorded (12000 - 4000 $cm^{-1}$).


## Centering and Scaling

Centering the data by substracting the mean is called normalizing the data, if the data is also scaled to unit variance the term standardization is used.
Standardized data have a mean of 0 and a standard deviation of 1. Standardising inputs is necesary when equally important features are meassured on different scales.


$$
\begin{array}{l}
X c_{i j}=X_{i j}-\bar{X}_{j} \\
X s_{i j}=\frac{X_{i j}-\bar{X}_{j}}{s_{j}}
\end{array}
$$

where $Xc$ and $Xs$ are the mean centered and auto-scaled matrices, $X$ is the input matrix, $Xj$ and $sj$ are the mean and standard deviation of variable $j$. [@Stevens2014]

In the pipeline this step is performed by a class called GlobalStandardScaler.


### Scatter corrections


When meassureing powders and solids in diffuse reflectance with NIR instruments, scattering artifacts are present in the spectra. Multiplicative scatter correction techniques are used to correct their occurence and improve linear behaviour of the obtained spectra.
[@Gallagher2000]


#### Multiplicative Scatter correction

MSC is a widely used normalization filter used in reflectance spectra to remove unwanted variance from scattering effects in powders or on surfaces. The algorithm filters chemically related variance and scattering related variance, leading to more interpretable normalized spectra.

1. Estimation of the correction coefficients (additive and multiplicative contributions resulting in baselineshifts and slope shifts)


$$
\mathbf{x}_{o r g}=b_{0}+b_{r e f, 1} \cdot \mathbf{x}_{r e f}+\mathbf{e}
$$

2. Correcting the recorded spectrum (estimating $b_{0}$, $b_{r e f, 1}$ for each spectra using OLS regression.)

$$
\mathbf{x}_{c o r r}=\frac{\mathbf{x}_{o r g}-b_{0}}{b_{r e f, 1}}=\mathbf{x}_{r e f}+\frac{\mathbf{e}}{b_{r e f, 1}}
$$

[@Rinnan2009]


## Spectral derivatives
Derivatives have the capability to remove both additive and multiplicative effects in the spectra and have been used in analytical spectroscopy for decades. [@Rinnan2009]

Derivating spectra can remove additive and multiplicative effects from NIR spectra, but can also amplify noise. They are often used to remove baseline offsets and are therefore a popular method in chemometrics and spectroscopy.

Advantage | Drawback
----------|----------
Reduce of baseline offset | Risk of overfitting the calibration model
Can resolve absorption overlapping | Increase noise, smoothing required 
Compensates for instrumental drift | Increase uncertainty in model coefficients
Enhances small spectral absorptions | Complicate spectral interpretation 
Can increase accuracy for complex datasets | Removes the baseline!
[@Stevens2014]

Derivatives tend to increase noise. This can be used to our advantage when determining regions of interest, high noise regions at the edges become clearly visible.

$$
\begin{array}{c}
x_{i}^{\prime}=x_{i}-x_{i-1} \\
x_{i}^{\prime \prime}=x_{i-1}-2 \cdot x_{i}+x_{i+1}
\end{array}
$$


## 2.3.1 Noise removal
Noise is a sum of not meassured variables [@Breiman2001] and appears in spectra as random fluctuations around the signal.

Noise can simply be reduced by performing $n$ repeated measurements of a sample, averaging out the  random fluctuations and variance from random effects by a factor of $\sqrt{n}$. Another way to remove noise is mathematically by a smoothing filter.

### Savitzky-Golay (savgol) Filter

One of the most commonly used and frequently cited filters in chemometrics is the Savitzky- Golay smoothing and differentiation filter.It fits a local polynomial regression on the signal and requires equidistant bandwidth. It smoothes the signal by performing a weighted sum on adjacent points.



 $$
x_{j} *=\frac{1}{N} \sum_{h=-k}^{k} c_{h} x_{j+h}
$$


* $xj∗$ new value
* $N$ normalizing coefficient 
* $k$ number of neighbour values
* $j$ and $c_{h}$ are precomputed coefficient for neighbouring wavelenghts of $k$
 $j$ $c_{h}$ depend on the polynomial order and degree (smoothing, first and second derivative)

## Deep Learning Preprocessing 


## Preprocessing

Preprocessing NIR spectra is an essential step prior to analysis when using traditional machine learning methods. However, the convolutional layers of the deep neural network are able to learn effective preprocessing methods, by minimizing the training and test set errors through backpropagation. The input data just needs to be standardized beforehand.
Visualizing the convolutional layers output, the transformed spectra resamble spectra treated with known algorithms, such as first derivative Savitzky-Golay filtering. The convolutional layers perform an optimized moving weighted average by tuning the parameters through backpropropagation by the optimizer function each training epoch until the cost function (mean squared error) reaches a minimum. [@Cui2018]

###ad pictuire###

Training a convolutional neural network on traditionally preprocessed spectra leads to suboptimal results, as the variation from which the convolutional layers can learn is reduced. 

Another problem is that deep neural networks need lots of training data to learn meaningful relationships from the spectra, otherwise it may map the position of a feature, and fails to recognise it in another location or from a different perspective. One simple but effective trick for spatial data processed by CNN is to generate more training data by dataaugmentation. Dataaugmentation is traditionally performed on pictures, which are turned, cropped and mirrored to make the network recognize the feature independently of its postion or angle.

In the case of NIR spectra, it is known that scattering artefacts from meassureing in diffuse reflectance results in randomly varrying slopes, intensities, baselines and multiplicative effects.
By simulating these variations, resulting from physical not chemical properties, we can manyfold our training samples and smooth out some of the created physical noise using the Extended Multiplicative Scattercorrection algorithm EMSC. The resulting training data set, still containing some simulated variance, leads to a more robust and fine tuned model, than training on scatter corrected or augmented data alone. [@Bjerrum2017]


```python
aug_pipline = Pipeline([
    ("scaleing_X", GlobalStandardScaler()),
    ("dataaugmentation", Dataaugument()),
    ("scatter_correction", EmscScaler())
    ])    
```


 ### DataAugmentation

In the case of NIR spectra, it is known that scattering artefacts from meassureing in diffuse reflectance results in randomly varrying slopes, intensities, baselines and multiplicative effects.
By simulating these variations, resulting from physical not chemical properties, we can manyfold our training samples and smooth out some of the created physical noise using the Extended Multiplicative Scattercorrection algorithm. The resulting training data set, still containing some simulated variance, leads to a more robust and fine tuned model, than training on scatter corrected or augmented data alone. [@Bjerrum2017]

 #####
Dataaugmentation makes it possible to multiply the training data by a factor of for example 100  and increases training succes greatly, especially if their is not a lot of traingdata. 

We call the augmented trainingsdata X_aug and y_aug. The reference method y_aug is simply multiplied by 100. The trainingsspectra on the other hand, are transformed to simulate noise from diffuse reflectance meassurements and smoothed afterwards using Extended Multiplicative Scattercorrection. The augmentated spectra improve the convolutionallayers ability to learn effective preprocessing transformations and improve overall feature extraktion.


 #####


The proposed method from [@Bjerrum2017] to apply dataaugmentaion on NIR spectra is shown in the below code block.
Instead of correctingthe baseline variations we use a special implementation of data augmentation. The idea is to simulate the expected form of physical noise (here baseline offset and slope and overall spectrum intensity), and expect the neural network to either extract features robust to the variations, or figure out the best corrections during training. The idea is similar to the image rotations and croppings done in image recognition training. [@Bjerrum2017]

We use the augmented data only for training. The redundant data, especially the simply repeated reponse variable bring no benefit for adjusting the errors during backpropagation.

```python

	def augment(self, X, y= None):
    '''Using dataaugmentation on input spectra to create slightly altered spectra to train NeuralNet'''
		self.betashift
		self.slopeshift
		self.multishift
		#Shift of baseline
		#calculate arrays
		beta = np.random.random(size=(X.shape[0],1))*2*self.betashift-self.betashift
		slope = np.random.random(size=(X.shape[0],1))*2*self.slopeshift-self.slopeshift + 1
		#Calculate relative position
		axis = np.array(range(X.shape[1]))/float(X.shape[1])
		#Calculate offset to be added
		offset = slope*(axis) + beta - axis - slope/2. + 0.5

		#Multiplicative
		multi = np.random.random(size=(X.shape[0],1))*2*self.multishift-self.multishift + 1

		X = multi*X + offset

		return X
```
The above function is implemented as a class containing fit, fit_transform and transform method, to use the scikit-learn Piplene. It is important to note that only the spectra used for training the CNN are dataaugmented.

#### add some pictures


_The fat blue line is the original spectrum and the others are the modified ones. Lets apply this to all the spectrums and expand the dataset with a factor of 100. I've found that the offset and multiplication works OK around 10% of the standard deviation for the whole dataset. The slopeshift is set to half of this. This could vary depending on the dataset and may be worth experimenting with._










