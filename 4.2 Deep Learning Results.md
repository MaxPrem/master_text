---
attachments: [Clipboard_2020-07-30-11-01-33.png, Clipboard_2020-07-30-11-01-48.png, Clipboard_2020-07-30-11-01-53.png, Clipboard_2020-07-31-16-00-52.png, Clipboard_2020-07-31-16-24-30.png]
tags: [masterarbeit]
title: 4.2 Deep Learning Results
created: '2020-07-24T18:19:14.312Z'
modified: '2020-08-06T18:18:30.660Z'
---

# 4.2 Deep Learning Results

# Preprocessing results

Deep Neural Networks with millions of trainable parameters need a lot of data to learn meaningful relationships, instead of patterns found in random noise. The bigger and deeper the network, the better the network can emulate the process that leads to the generation of the data. With too many parameters to tune and too few samples to train on, the network is prone to overfitting. Regularization techniques as dataaugmentation, Dropout and early stopping try to mitigate overfitting.

Methods like Dropout prevent overfitting by randomly deactivating 50% of the neurons during training. This has the effekt that the neurons become more robust to noise and predict more independently. Early stopping is ending the model's training when no more improvement can be achieved after a defined training period. 

Last but not least dataaugmentation makes it possible to train deep convolutional networks on small training sets without overfitting.

One of these prerequisites is to preprocess the NIR spectra in order to remove unwanted physical artefacts obtained from measurements in diffuse reflectance. The resulting additive and multiplicative effects from reflection on surfaces of solids and powders appear in the form of varying baselines, slopes, intensities and other random fluctuations.

This is achieved by multiplying the training data by a factor of 100 and emulating noise not related to chemical properties, resulting from measurements in diffuse reflectance. The simulated additive and multiplicative effects from reflections on surfaces of solids and powders appear in the form of varying baselines, slopes, intensities and other random fluctuations, resulting in a training set containing thousands of spectra. 

We call the augmented training data X_aug and y_aug. The reference method y_aug is simply multiplied by the same amount as the according spectra, that are transformed to simulate noise from measurement and smoothed afterwards using Extended Multiplicative Scatter Correction. The augmented spectra improve the convolutional layers ability to learn effective preprocessing transformations and improve overall feature extraction.

### Preprocessing with Data Augmentation

```python

yscaler = GlobalStandardScaler()
y_train = yscaler.fit_transform(y_train)
# y augmentation
y_aug = np.repeat(y_train, repeats=100, axis=0) #y_train is simply repeated

aug_pipline = Pipeline([
	("scaleing_X", GlobalStandardScaler()),
	("dataaugmentation", Dataaugument()),
	("scatter_correction", EmscScaler())
	])

# transformed and augmented X
X_aug = aug_pipline.fit_transform(X_train)
# transformed data
X_train = aug_pipline.transform(X_train)
X_val = aug_pipline.transform(X_val)
X_test = aug_pipline.transform(X_test)

data = {"X": X, "y": y, "X_test": X_test, "y_test": y_test, "X_val": X_val, "y_val": y_val}
```


### Model Architecture

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise (GaussianNois (None, 1951)              0         
_________________________________________________________________
reshape (Reshape)            (None, 1951, 1)           0         
_________________________________________________________________
separable_conv1d (SeparableC (None, 1951, 8)           40        
_________________________________________________________________
activation (selu)            (None, 1951, 8)           0         
_________________________________________________________________
mc_dropout (MCDropout)       (None, 1951, 8)           0         
_________________________________________________________________
separable_conv1d_1 (Separable (None, 1951, 16)          384       
_________________________________________________________________
activation_1 (selu)          (None, 1951, 16)          0         
_________________________________________________________________
flatten (Flatten)            (None, 31216)             0         
_________________________________________________________________
mc_dropout_1 (MCDropout)     (None, 31216)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               3995776   
_________________________________________________________________
activation_2 (selu)          (None, 128)               0         
_________________________________________________________________
mc_dropout_2 (MCDropout)     (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 128       
_________________________________________________________________
activation_3 (linear)        (None, 1)                 0         
=================================================================
Total params: 3,996,328
Trainable params: 3,996,328
Non-trainable params: 0
________________________

```
```
 model.compile(loss="mse", optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999))
    return model
```

Training the network requires no tuning of hyperparameters. If the test set error fluctuates too much and no improvement occurs on the test data the number of test samples should be increased when performing sklearns train_test_split() to create the datasets. The test set is important to adjust the network errors and can also be overfit by the network.

![](@attachment/Clipboard_2020-07-30-11-01-33.png)

 The fluctuation in the training data is normal due to the small sample size, but training is successful as new minima are reached from time to time. If no improvement can be made on the training data after a defined period, the early_stopping callback stops the network from training.

### Validation Table using MonteCarloDropout

Training multiple models on different splits of 

```
*** Summary Reference Method: XP [gXP/kgTM] ***
Number of wavelengths: 1951
Number of training samples: 47
Number of test samples: 16
Number of validation samples: 16
*** NIR Metrics ***
Standard Error Prediction (SEP): 	4.84
___Benchmarks___
Scores   	train		test 		 val 
Var.expl.	0.96		0.965		0.95
R2       	0.953		0.965		0.943
MSE     	0.0465	0.0239	0.044
RMSE     	0.216		0.154		0.21
Huber    	0.0225	0.0117	0.0208
___MC dropout benchmarks___
Scores   	train		test 		 val 
Var.expl. 0.961	  0.966		0.951
R2       	0.954		0.966		0.944
MSE      	0.0462	0.0235	0.0431
RMSE     	0.215	  0.153		0.208
Huber    	0.0224	0.0115	0.0204
```

### Regression plot with error bars
The prediction intervals were obtained by repeatedly predicting each sample 100 times using monte carlo dropout, randomly deactivating 50% of the weights for each prediction.
The resulting error bars from the calculated variances indicate that the samples in the center show a smaller prediction interval. The sample predictions in the left side corner, seem to be more accurate than the ones in the top corner. Indicating that there is a slightly higher level of uncertainty predicting the samples with higher concentration. This is also stated by the blue approximate line, generated using mc-dropout predictions and multiple linear regression, deviating from the perfect diagonal position of the red line. Overall the plot shows an almost perfect regression.


![](@attachment/Clipboard_2020-07-31-16-00-52.png)


### Results from Tensorboard

The training can be monitored using tensorboard.
```
tensorboard --logdir=./my_logs --port=6006
> Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
> TensorBoard 2.0.1 at http://localhost:6006/ (Press CTRL+C to quit)
```

![](@attachment/Clipboard_2020-07-31-16-24-30.png)






