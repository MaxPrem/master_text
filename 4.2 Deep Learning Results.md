---
attachments: [Clipboard_2020-07-30-11-01-33.png, Clipboard_2020-07-30-11-01-48.png, Clipboard_2020-07-30-11-01-53.png, Clipboard_2020-07-31-16-00-52.png, Clipboard_2020-07-31-16-24-30.png]
tags: [masterarbeit]
title: 4.2 Deep Learning Results
created: '2020-07-24T18:19:14.312Z'
modified: '2020-08-03T23:10:03.526Z'
---

# 4.2 Deep Learning Results

# Preprocessing results

Deep Neural Networks with millions of trainable parameters need a lot of data to learn meaningful relationships, instead of patterns found in random noise. The bigger and deeper the network, the better the network can emulate the process that lead to the generation of the data. With too many parameters to tune and too few samples to train on, the network is prone to overfitting. Regularization techniques as dataaugmentation, Dropout and early stopping try to mitigate overfitting.

Methods like Dropout prevent overfitting by randomly deactivating 50% of the neurons during training. This has the effekt that the neurons become more robust to noise and predict more independtly. Early stopping is ending the models training when no more improvement can be achieved after a defined training period. 

Last but not least dataaugmentation makes it possible to train deep convolutional networks on small trainingsets without overfitting.

One of these prequesites is to preprocess the NIR spectra in order to remove unwanted physical artefacts obtained from meassureing in diffuse reflectance. The resulting additive and multiplicative effects from reflection on surfaces of solids and powders appear in the form of varying baselines, slopes, intensities and other random fluctuations.

This is achieved by multiplying the trainingsdata by a factor of 100 and emulating noise not related to chemical properties, resulting from meassureing in diffuse reflectance. The simulated additive and multiplicative effects from reflections on surfaces of solids and powders appear in the form of varying baselines, slopes, intensities and other random fluctuations, resulting in a training set containg thousands of spectra. 

We call the augmented trainingsdata X_aug and y_aug. The reference method y_aug is simply multiplied by the same amount as the according spectra, that are transformed to simulat noise from meassurement and smoothed afterwards using Extended Multiplicative Scattercorrection. The augmentated spectra improve the convolutionallayers ability to learn effective preprocessing transformations and improve overall feature extraktion.

### Preprocessing with Dataaugmentation

```python

yscaler = GlobalStandardScaler()
y_train = yscaler.fit_transform(y_train)
# y augmentation
y_aug = np.repeat(y_train, repeats=100, axis=0) #y_train is simply repeated

aug_pipline = Pipeline([
	("scaleing_X", GlobalStandardScaler()),
	("dataaugmentation", Dataaugument()),
	("scatter_correction", EmscScaler())
	])

# transformed and augmentated X
X_aug = aug_pipline.fit_transform(X_train)
# transformed data
X_train = aug_pipline.transform(X_train)
X_val = aug_pipline.transform(X_val)
X_test = aug_pipline.transform(X_test)

data = {"X": X, "y": y, "X_test": X_test, "y_test": y_test, "X_val": X_val, "y_val": y_val}
```


### Model Architecture

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise (GaussianNois (None, 1951)              0         
_________________________________________________________________
reshape (Reshape)            (None, 1951, 1)           0         
_________________________________________________________________
separable_conv1d (SeparableC (None, 1951, 8)           40        
_________________________________________________________________
activation (selu)            (None, 1951, 8)           0         
_________________________________________________________________
mc_dropout (MCDropout)       (None, 1951, 8)           0         
_________________________________________________________________
separable_conv1d_1 (Separabl (None, 1951, 16)          384       
_________________________________________________________________
activation_1 (selu)          (None, 1951, 16)          0         
_________________________________________________________________
flatten (Flatten)            (None, 31216)             0         
_________________________________________________________________
mc_dropout_1 (MCDropout)     (None, 31216)             0         
_________________________________________________________________
dense (Dense)                (None, 128)               3995776   
_________________________________________________________________
activation_2 (selu)          (None, 128)               0         
_________________________________________________________________
mc_dropout_2 (MCDropout)     (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 128       
_________________________________________________________________
activation_3 (linear)        (None, 1)                 0         
=================================================================
Total params: 3,996,328
Trainable params: 3,996,328
Non-trainable params: 0
________________________

```
```
 model.compile(loss="mse", optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999))
    return model
```

Training the network requires no tuneing of hyperparameters. If the test set error is fluctuation to much and no improvement occurs on the test data the number of test samples should be increased when performing sklearns train_test_split() to create the datasets. The test set is important to adjust the network errors and can also be overfit by the network.

![](@attachment/Clipboard_2020-07-30-11-01-33.png)

 The fluctuation in the trainig data is normal due to the small sample size, but training is succesful as new minima are reaches from time to time. If no improvement can be made on the training data after a defined period, the early_stopping callback stops the network from training.

### Validation Table using MonteCarloDropout

Training multiple models on different splits of 

```
*** Summary Reference Method: XP [gXP/kgTM] ***
Number of wavelengths: 1951
Number of training sampels: 47
Number of test sampels: 16
Number of validation sampels: 16
*** NIR Metrics ***
Standard Error Prediction (SEP): 	4.84
___Benchmarks___
Scores   	train		test 		 val 
Var.expl.	0.96		0.965		0.95
R2       	0.953		0.965		0.943
MSE     	0.0465	0.0239	0.044
RMSE     	0.216		0.154		0.21
Huber    	0.0225	0.0117	0.0208
___MC dropout benchmarks___
Scores   	train		test 		 val 
Var.expl. 0.961	  0.966		0.951
R2       	0.954		0.966		0.944
MSE      	0.0462	0.0235	0.0431
RMSE     	0.215	  0.153		0.208
Huber    	0.0224	0.0115	0.0204
```

### Regression plot with errobars
The prediction intervalls were obtained by repeadetly predciting each sample 100 times using monte carlo dropout, randomly deactivate 50% of the weights for each prediction.
The resulting errobars from the calculated variances indicate that the samples in the center show a smaller prediction intervall. The sample predictions in the left side corner, seem to be more accurate than the ones in the top corner. Indicating that there is a slightly higher level of uncertainty predicting the samples with higher concentration. This is also stated by the blue aproximated line, generated using mc-dropout predictions and multiple linear regression, deviating from the perfect diagonal position of the red line. Overall the plot shows an almost perfect regression.


![](@attachment/Clipboard_2020-07-31-16-00-52.png)


### Results from Tensorboard

The training can be monitored using tensorboard.
```
tensorboard --logdir=./my_logs --port=6006
> Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
> TensorBoard 2.0.1 at http://localhost:6006/ (Press CTRL+C to quit)
```

![](@attachment/Clipboard_2020-07-31-16-24-30.png)




